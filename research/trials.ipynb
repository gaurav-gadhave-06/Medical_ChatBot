{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\AI & ML\\\\Gen AI\\\\Gen-AI-Projects\\\\End-to-end-Medical-Chatbot-Generative-AI-main\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\AI & ML\\\\Gen AI\\\\Gen-AI-Projects\\\\End-to-end-Medical-Chatbot-Generative-AI-main'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf_file(data='Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 5860\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "GROQ_API_KEY=os.environ.get('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while installing plugin inference: can't set attribute 'inference'\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\AI & ML\\Gen AI\\Gen-AI-Projects\\End-to-end-Medical-Chatbot-Generative-AI-main\\venv\\lib\\site-packages\\pinecone_plugin_interface\\actions\\installation.py\", line 13, in install_plugins\n",
      "    setattr(target, plugin.namespace, impl(target.config, plugin_client_builder))\n",
      "AttributeError: can't set attribute 'inference'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"medicalbot\",\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"medicalbot-pfqor2w.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 384,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"medicalbot\"\n",
    "\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\", \n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while installing plugin inference: can't set attribute 'inference'\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\AI & ML\\Gen AI\\Gen-AI-Projects\\End-to-end-Medical-Chatbot-Generative-AI-main\\venv\\lib\\site-packages\\pinecone_plugin_interface\\actions\\installation.py\", line 13, in install_plugins\n",
      "    setattr(target, plugin.namespace, impl(target.config, plugin_client_builder))\n",
      "AttributeError: can't set attribute 'inference'\n"
     ]
    }
   ],
   "source": [
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while installing plugin inference: can't set attribute 'inference'\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\AI & ML\\Gen AI\\Gen-AI-Projects\\End-to-end-Medical-Chatbot-Generative-AI-main\\venv\\lib\\site-packages\\pinecone_plugin_interface\\actions\\installation.py\", line 13, in install_plugins\n",
      "    setattr(target, plugin.namespace, impl(target.config, plugin_client_builder))\n",
      "AttributeError: can't set attribute 'inference'\n"
     ]
    }
   ],
   "source": [
    "# Load Existing index \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x167528c2590>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is Acne?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1e67efe2-69c6-4149-acf0-bfe84703a5b8', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='4b41f2f6-ee33-4b7d-b50a-3fb1feccdbf8', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 38.0, 'page_label': '39', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a womanâ€™s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed.(Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25'),\n",
       " Document(id='bb3cfa27-4127-408f-84d5-ba2bd1106973', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 37.0, 'page_label': '38', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='Acidosis see Respiratory acidosis; Renal\\ntubular acidosis; Metabolic acidosis\\nAcne\\nDefinition\\nAcne is a common skin disease characterized by\\npimples on the face, chest, and back. It occurs when the\\npores of the skin become clogged with oil, dead skin\\ncells, and bacteria.\\nDescription\\nAcne vulgaris, the medical term for common acne, is\\nthe most common skin disease. It affects nearly 17 million\\npeople in the United States. While acne can arise at any')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-groq\n",
      "  Using cached langchain_groq-0.2.4-py3-none-any.whl (14 kB)\n",
      "Collecting groq<1,>=0.4.1\n",
      "  Using cached groq-0.18.0-py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from langchain-groq) (0.3.40)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.8.0)\n",
      "Requirement already satisfied: sniffio in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (2.10.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-groq) (0.3.11)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-groq) (24.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-groq) (1.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-groq) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-groq) (6.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-groq) (0.23.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-groq) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-groq) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-groq) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ai & ml\\gen ai\\gen-ai-projects\\end-to-end-medical-chatbot-generative-ai-main\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-groq) (2.3.0)\n",
      "Installing collected packages: groq, langchain-groq\n",
      "Successfully installed groq-0.18.0 langchain-groq-0.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'D:\\AI & ML\\Gen AI\\Gen-AI-Projects\\End-to-end-Medical-Chatbot-Generative-AI-main\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(temperature=0.4, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acromegaly is a disorder caused by the abnormal release of a chemical from the pituitary gland, resulting in increased growth in bone and soft tissue. It is a rare condition, affecting approximately 50 out of every one million people. The symptoms, such as unusual height and enlarged facial features, develop gradually and often lead to delayed diagnosis.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A complete blood count (CBC) is a series of tests that evaluates the cells suspended in the liquid part of blood, providing valuable information about the blood and other body systems. Abnormal results can indicate conditions such as anemias, leukemias, and infections. It is a useful screening and diagnostic test.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is stats?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
